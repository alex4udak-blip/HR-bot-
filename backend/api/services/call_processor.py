"""
Call Processor Service - Transcribes audio and analyzes content.
Uses OpenAI Whisper for transcription and Claude for analysis.
"""

import asyncio
import logging
import os
import json
import glob
import tempfile
import shutil
import subprocess
from datetime import datetime
from typing import Optional

from openai import AsyncOpenAI
from anthropic import AsyncAnthropic

from ..config import settings

logger = logging.getLogger("hr-analyzer.call_processor")

# OpenAI Whisper has a 25MB file size limit
# Using 24MB as a safe threshold
MAX_FILE_SIZE = 24 * 1024 * 1024  # 24MB in bytes


class CallProcessor:
    """Processes call recordings: transcription + AI analysis."""

    def __init__(self):
        self.openai = None
        self.anthropic = None

    def _init_clients(self):
        """Initialize API clients lazily."""
        if self.openai is None and settings.openai_api_key:
            self.openai = AsyncOpenAI(api_key=settings.openai_api_key)
        if self.anthropic is None and settings.anthropic_api_key:
            self.anthropic = AsyncAnthropic(api_key=settings.anthropic_api_key)

    async def process_call(self, call_id: int):
        """Full processing pipeline for a call recording."""
        from ..database import AsyncSessionLocal
        from ..models.database import CallRecording, CallStatus
        from sqlalchemy import select

        self._init_clients()

        async with AsyncSessionLocal() as db:
            result = await db.execute(
                select(CallRecording).where(CallRecording.id == call_id)
            )
            call = result.scalar_one_or_none()

            if not call:
                logger.error(f"Call {call_id} not found")
                return

            if not call.audio_file_path:
                logger.error(f"Call {call_id} has no audio file")
                call.status = CallStatus.failed
                call.error_message = "No audio file"
                await db.commit()
                return

            try:
                # 1. Convert to WAV if needed
                audio_path = await self._convert_to_wav(call.audio_file_path)

                # 2. Transcribe via Whisper
                call.status = CallStatus.transcribing
                await db.commit()

                transcript = await self._transcribe(audio_path)
                call.transcript = transcript

                # 3. Get audio duration
                duration = await self._get_duration(call.audio_file_path)
                call.duration_seconds = duration

                # 4. AI analysis
                call.status = CallStatus.analyzing
                await db.commit()

                analysis = await self._analyze(transcript)
                call.summary = analysis.get("summary")
                call.action_items = analysis.get("action_items")
                call.key_points = analysis.get("key_points")

                # Use formatted transcript with speaker labels if available
                if analysis.get("formatted_transcript"):
                    call.transcript = analysis.get("formatted_transcript")

                # Done!
                call.status = CallStatus.done
                call.processed_at = datetime.utcnow()
                await db.commit()

                logger.info(f"Call {call_id} processed successfully")

                # Cleanup converted file if different from original
                if audio_path != call.audio_file_path and os.path.exists(audio_path):
                    os.remove(audio_path)

            except Exception as e:
                logger.error(f"Error processing call {call_id}: {e}")
                call.status = CallStatus.failed
                call.error_message = str(e)
                await db.commit()

    async def _convert_to_wav(self, input_path: str) -> str:
        """Convert audio to WAV 16kHz mono for Whisper."""
        if input_path.endswith('.wav'):
            return input_path

        output_path = input_path.rsplit('.', 1)[0] + '_converted.wav'

        cmd = [
            'ffmpeg', '-i', input_path,
            '-ar', '16000',  # 16kHz sample rate
            '-ac', '1',       # mono
            '-y',             # overwrite
            output_path
        ]

        try:
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, stderr = await process.communicate()

            if process.returncode != 0:
                logger.warning(f"ffmpeg conversion warning: {stderr.decode()}")
                # Return original if conversion fails
                return input_path

            return output_path

        except Exception as e:
            logger.warning(f"Conversion failed, using original: {e}")
            return input_path

    async def _transcribe(self, audio_path: str) -> str:
        """
        Transcribe audio using OpenAI Whisper API.
        For large files (>24MB), automatically splits into chunks and combines transcripts.
        """
        if not self.openai:
            raise ValueError("OpenAI API key not configured")

        file_size = os.path.getsize(audio_path)

        # If file is small enough, transcribe directly
        if file_size <= MAX_FILE_SIZE:
            return await self._transcribe_single_file(audio_path)

        # Large file: split into chunks and transcribe each
        logger.info(f"Large file detected ({file_size / 1024 / 1024:.1f}MB), splitting into chunks...")
        chunk_files = await self._split_audio_chunks(audio_path)

        try:
            transcripts = []
            for i, chunk_path in enumerate(chunk_files):
                logger.info(f"Transcribing chunk {i + 1}/{len(chunk_files)}...")
                chunk_size = os.path.getsize(chunk_path)

                if chunk_size > MAX_FILE_SIZE:
                    # Re-split this chunk with smaller duration
                    sub_chunks = await self._split_audio_chunks(chunk_path, chunk_duration_sec=300)
                    try:
                        for sub_chunk in sub_chunks:
                            transcript = await self._transcribe_single_file(sub_chunk)
                            transcripts.append(transcript)
                    finally:
                        sub_dir = os.path.dirname(sub_chunks[0]) if sub_chunks else None
                        if sub_dir and os.path.exists(sub_dir):
                            shutil.rmtree(sub_dir)
                else:
                    transcript = await self._transcribe_single_file(chunk_path)
                    transcripts.append(transcript)

            # Combine all transcripts into one
            combined = " ".join(transcripts)
            logger.info(f"Successfully combined {len(transcripts)} transcript chunks")
            return combined

        finally:
            # Clean up chunk files
            chunk_dir = os.path.dirname(chunk_files[0]) if chunk_files else None
            if chunk_dir and os.path.exists(chunk_dir):
                shutil.rmtree(chunk_dir)

    async def _transcribe_single_file(self, audio_path: str) -> str:
        """Transcribe a single audio file (must be under 25MB)."""
        with open(audio_path, 'rb') as f:
            response = await self.openai.audio.transcriptions.create(
                model="whisper-1",
                file=f,
                language="ru"
            )
        return response.text

    async def _split_audio_chunks(self, audio_path: str, chunk_duration_sec: int = 600) -> list:
        """
        Split audio file into chunks using FFmpeg.

        Args:
            audio_path: Path to the audio file
            chunk_duration_sec: Duration of each chunk in seconds (default 10 minutes)

        Returns:
            List of paths to chunk files
        """
        temp_dir = tempfile.mkdtemp(prefix="audio_chunks_")

        _, ext = os.path.splitext(audio_path)
        if not ext:
            ext = ".wav"

        output_pattern = os.path.join(temp_dir, f"chunk_%03d{ext}")

        try:
            process = await asyncio.create_subprocess_exec(
                "ffmpeg", "-i", audio_path,
                "-f", "segment",
                "-segment_time", str(chunk_duration_sec),
                "-c", "copy",
                "-y", output_pattern,
                stdout=subprocess.DEVNULL, stderr=subprocess.PIPE,
            )
            _, stderr = await process.communicate()

            if process.returncode != 0:
                stderr_text = stderr.decode() if stderr else ""
                raise Exception(f"FFmpeg split failed: {stderr_text[:200]}")

            chunk_pattern = os.path.join(temp_dir, f"chunk_*{ext}")
            chunk_files = sorted(glob.glob(chunk_pattern))

            if not chunk_files:
                raise Exception("No chunks were created")

            logger.info(f"Split audio into {len(chunk_files)} chunks")
            return chunk_files

        except Exception as e:
            if os.path.exists(temp_dir):
                shutil.rmtree(temp_dir)
            raise e

    async def _get_duration(self, audio_path: str) -> int:
        """Get audio duration in seconds using ffprobe."""
        cmd = [
            'ffprobe', '-i', audio_path,
            '-show_entries', 'format=duration',
            '-v', 'quiet', '-of', 'csv=p=0'
        ]

        try:
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, _ = await process.communicate()

            return int(float(stdout.decode().strip()))
        except Exception:
            return 0

    async def analyze_transcript(
        self,
        call_id: int,
        transcript: str,
        speakers: list,
        fireflies_summary: Optional[dict] = None
    ):
        """
        Analyze a transcript from Fireflies (already transcribed with speaker diarization).

        Args:
            call_id: Internal call ID
            transcript: Formatted transcript text
            speakers: List of speaker segments with timestamps
            fireflies_summary: Optional summary from Fireflies
        """
        from ..database import AsyncSessionLocal
        from ..models.database import CallRecording, CallStatus
        from sqlalchemy import select

        self._init_clients()

        async with AsyncSessionLocal() as db:
            result = await db.execute(
                select(CallRecording).where(CallRecording.id == call_id)
            )
            call = result.scalar_one_or_none()

            if not call:
                logger.error(f"Call {call_id} not found for transcript analysis")
                return

            try:
                call.status = CallStatus.analyzing
                call.transcript = transcript
                call.speakers = speakers
                await db.commit()

                # Use Fireflies summary if available, otherwise use Claude
                if fireflies_summary and fireflies_summary.get("overview"):
                    analysis = {
                        "summary": fireflies_summary.get("overview", ""),
                        "key_points": fireflies_summary.get("keywords", []),
                        "action_items": fireflies_summary.get("action_items", [])
                    }
                    logger.info(f"Using Fireflies summary for call {call_id}")
                else:
                    # Fallback to Claude analysis
                    analysis = await self._analyze(transcript)
                    logger.info(f"Used Claude analysis for call {call_id}")

                call.summary = analysis.get("summary")
                call.action_items = analysis.get("action_items")
                call.key_points = analysis.get("key_points")
                call.status = CallStatus.done
                call.processed_at = datetime.utcnow()
                await db.commit()

                logger.info(f"Call {call_id} transcript analyzed successfully")

            except Exception as e:
                logger.error(f"Error analyzing transcript for call {call_id}: {e}")
                call.status = CallStatus.failed
                call.error_message = str(e)
                await db.commit()

    async def _analyze(self, transcript: str) -> dict:
        """Analyze transcript using Claude. Handles long transcripts by chunking."""
        if not self.anthropic:
            logger.warning("Anthropic API key not configured, skipping analysis")
            return {
                "summary": "Analysis not available (API key not configured)",
                "key_points": [],
                "action_items": []
            }

        # For very long transcripts, analyze in chunks and combine
        CHUNK_SIZE = 50000  # Characters per chunk
        MAX_SINGLE_ANALYSIS = 60000  # If less than this, analyze as single piece

        if len(transcript) <= MAX_SINGLE_ANALYSIS:
            # Short transcript - analyze directly
            return await self._analyze_single(transcript)
        else:
            # Long transcript - chunk and combine
            logger.info(f"Long transcript ({len(transcript)} chars), using chunked analysis")
            return await self._analyze_chunked(transcript, CHUNK_SIZE)

    async def _analyze_chunked(self, transcript: str, chunk_size: int) -> dict:
        """Analyze long transcript by splitting into chunks and combining."""
        # Split transcript into chunks
        chunks = []
        for i in range(0, len(transcript), chunk_size):
            chunk = transcript[i:i + chunk_size]
            chunks.append(chunk)

        logger.info(f"Split transcript into {len(chunks)} chunks")

        # Analyze each chunk
        chunk_analyses = []
        for i, chunk in enumerate(chunks):
            logger.info(f"Analyzing chunk {i + 1}/{len(chunks)}")
            chunk_prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ß–ê–°–¢–¨ {i + 1} –∏–∑ {len(chunks)} —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ —Å–æ–∑–≤–æ–Ω–∞.

–í–ê–ñ–ù–û: –≠—Ç–æ —Ç–æ–ª—å–∫–æ —á–∞—Å—Ç—å —Ä–∞–∑–≥–æ–≤–æ—Ä–∞. –ò–∑–≤–ª–µ–∫–∏ –í–°–ï –¥–µ—Ç–∞–ª–∏ –∏–∑ —ç—Ç–æ–π —á–∞—Å—Ç–∏:
- –í—Å–µ –æ–±—Å—É–∂–¥–∞–µ–º—ã–µ —Ç–µ–º—ã –∏ –ø–æ–¥—Ç–µ–º—ã
- –í—Å–µ —É–ø–æ–º—è–Ω—É—Ç—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏, –∫—Ä–∏—Ç–µ—Ä–∏–∏
- –í—Å–µ –∏–º–µ–Ω–∞, —Ü–∏—Ñ—Ä—ã, –ø—Ä–∏–º–µ—Ä—ã
- –í—Å–µ –∏–¥–µ–∏ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
- –í—Å–µ –¥–æ–≥–æ–≤–æ—Ä—ë–Ω–Ω–æ—Å—Ç–∏ –∏ –∑–∞–¥–∞—á–∏

–¢–ê–ö–ñ–ï: –ü—Ä–µ–æ–±—Ä–∞–∑—É–π —ç—Ç—É —á–∞—Å—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ –≤ –¥–∏–∞–ª–æ–≥ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —Å–ø–∏–∫–µ—Ä–æ–≤:
- –û–ø—Ä–µ–¥–µ–ª–∏ —Å–ø–∏–∫–µ—Ä–æ–≤ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É (–≤–æ–ø—Ä–æ—Å—ã vs –æ—Ç–≤–µ—Ç—ã)
- –ò—Å–ø–æ–ª—å–∑—É–π —Ä–æ–ª–∏: "HR:", "–ö–∞–Ω–¥–∏–¥–∞—Ç:", "–ú–µ–Ω–µ–¥–∂–µ—Ä:", "–°–ø–∏–∫–µ—Ä 1:", "–°–ø–∏–∫–µ—Ä 2:"
- –°–æ—Ö—Ä–∞–Ω–∏ –í–°–ï —Å–ª–æ–≤–∞, —Ç–æ–ª—å–∫–æ –¥–æ–±–∞–≤—å –º–µ—Ç–∫–∏ —Å–ø–∏–∫–µ—Ä–æ–≤

–ß–∞—Å—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞:
---
{chunk}
---

–û—Ç–≤–µ—Ç—å JSON:
{{
  "topics": ["—Ç–µ–º–∞ 1", "—Ç–µ–º–∞ 2", ...],
  "details": ["–≤–∞–∂–Ω–∞—è –¥–µ—Ç–∞–ª—å 1", "–≤–∞–∂–Ω–∞—è –¥–µ—Ç–∞–ª—å 2", ...],
  "key_points": ["–∫–ª—é—á–µ–≤–æ–π –º–æ–º–µ–Ω—Ç 1", "–∫–ª—é—á–µ–≤–æ–π –º–æ–º–µ–Ω—Ç 2", ...],
  "action_items": ["–∑–∞–¥–∞—á–∞ 1", "–∑–∞–¥–∞—á–∞ 2", ...],
  "profiles": ["–µ—Å–ª–∏ –æ–±—Å—É–∂–¥–∞–ª–∏—Å—å —Ä–æ–ª–∏/–∫–∞–Ω–¥–∏–¥–∞—Ç—ã - –≤—Å–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏"],
  "decisions": ["—Ä–µ—à–µ–Ω–∏–µ 1", "—Ä–µ—à–µ–Ω–∏–µ 2", ...],
  "open_questions": ["–Ω–µ—Ä–µ—à—ë–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å 1", ...],
  "formatted_transcript": "HR: —Ä–µ–ø–ª–∏–∫–∞...\\n–ö–∞–Ω–¥–∏–¥–∞—Ç: —Ä–µ–ø–ª–∏–∫–∞..."
}}"""

            try:
                response = await self.anthropic.messages.create(
                    model="claude-sonnet-4-20250514",
                    max_tokens=8000,
                    messages=[{"role": "user", "content": chunk_prompt}]
                )
                text = response.content[0].text
                start = text.find('{')
                end = text.rfind('}') + 1
                if start != -1 and end > start:
                    chunk_analyses.append(json.loads(text[start:end]))
            except Exception as e:
                logger.error(f"Chunk {i + 1} analysis failed: {e}")

        # Combine all chunk analyses into final analysis
        return await self._combine_chunk_analyses(chunk_analyses, len(transcript))

    async def _combine_chunk_analyses(self, chunk_analyses: list, total_length: int) -> dict:
        """Combine multiple chunk analyses into a final comprehensive analysis."""
        if not chunk_analyses:
            return {"summary": "Analysis failed", "key_points": [], "action_items": []}

        # Collect all data from chunks
        all_topics = []
        all_details = []
        all_key_points = []
        all_action_items = []
        all_profiles = []
        all_decisions = []
        all_open_questions = []
        all_formatted_transcripts = []

        for analysis in chunk_analyses:
            all_topics.extend(analysis.get("topics", []))
            all_details.extend(analysis.get("details", []))
            all_key_points.extend(analysis.get("key_points", []))
            all_action_items.extend(analysis.get("action_items", []))
            all_profiles.extend(analysis.get("profiles", []))
            all_decisions.extend(analysis.get("decisions", []))
            all_open_questions.extend(analysis.get("open_questions", []))
            if analysis.get("formatted_transcript"):
                all_formatted_transcripts.append(analysis.get("formatted_transcript"))

        # Now create final synthesis
        synthesis_prompt = f"""–ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –í–°–ï–• —á–∞—Å—Ç–µ–π –¥–ª–∏–Ω–Ω–æ–≥–æ —Å–æ–∑–≤–æ–Ω–∞ ({total_length} —Å–∏–º–≤–æ–ª–æ–≤, {len(chunk_analyses)} —á–∞—Å—Ç–µ–π), —Å–æ–∑–¥–∞–π –§–ò–ù–ê–õ–¨–ù–´–ô –ò–°–ß–ï–†–ü–´–í–ê–Æ–©–ò–ô –∞–Ω–∞–ª–∏–∑.

–°–æ–±—Ä–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Å–µ—Ö —á–∞—Å—Ç–µ–π:

–¢–ï–ú–´: {json.dumps(all_topics, ensure_ascii=False)}

–î–ï–¢–ê–õ–ò: {json.dumps(all_details[:100], ensure_ascii=False)}

–ö–õ–Æ–ß–ï–í–´–ï –ú–û–ú–ï–ù–¢–´: {json.dumps(all_key_points, ensure_ascii=False)}

–ó–ê–î–ê–ß–ò: {json.dumps(all_action_items, ensure_ascii=False)}

–ü–†–û–§–ò–õ–ò/–†–û–õ–ò: {json.dumps(all_profiles, ensure_ascii=False)}

–†–ï–®–ï–ù–ò–Ø: {json.dumps(all_decisions, ensure_ascii=False)}

–û–¢–ö–†–´–¢–´–ï –í–û–ü–†–û–°–´: {json.dumps(all_open_questions, ensure_ascii=False)}

–°–æ–∑–¥–∞–π –§–ò–ù–ê–õ–¨–ù–´–ô —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑:

## SUMMARY:

**–°–£–¢–¨ –†–ê–ó–ì–û–í–û–†–ê** (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è):
–ì–ª–∞–≤–Ω–∞—è —Ü–µ–ª—å –∏ –∫–ª—é—á–µ–≤–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤—Å—Ç—Ä–µ—á–∏.

**–û–°–ù–û–í–ù–ê–Ø –¢–ï–ú–ê:**
–î–µ—Ç–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –≥–ª–∞–≤–Ω–æ–π —Ç–µ–º—ã.

**–ü–û–î–¢–ï–ú–´** (–≤—Å–µ —á—Ç–æ –æ–±—Å—É–∂–¥–∞–ª–æ—Å—å):
–ü–µ—Ä–µ—á–∏—Å–ª–∏ –í–°–ï –ø–æ–¥—Ç–µ–º—ã —Å –∫—Ä–∞—Ç–∫–∏–º –æ–ø–∏—Å–∞–Ω–∏–µ–º –∫–∞–∂–¥–æ–π.

**–î–ï–¢–ê–õ–¨–ù–´–ï –†–ê–ó–ë–û–†–´ –¢–ï–ú:**
–î–ª—è –∫–∞–∂–¥–æ–π –≤–∞–∂–Ω–æ–π —Ç–µ–º—ã/—Ä–æ–ª–∏/–ø—Ä–æ–µ–∫—Ç–∞:
- –ö–∞–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è/—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –Ω–∞–∑—ã–≤–∞–ª–∏—Å—å
- –ö–∞–∫–∏–µ –∏–¥–µ–∏ –ø—Ä–µ–¥–ª–∞–≥–∞–ª–∏—Å—å
- –ö–∞–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã –ø—Ä–∏–≤–æ–¥–∏–ª–∏—Å—å

**–ö –ß–ï–ú–£ –ü–†–ò–®–õ–ò (–¥–æ–≥–æ–≤–æ—Ä—ë–Ω–Ω–æ—Å—Ç–∏):**
–í–°–ï —Ä–µ—à–µ–Ω–∏—è –∏ –¥–æ–≥–æ–≤–æ—Ä—ë–Ω–Ω–æ—Å—Ç–∏.

**–ö –ß–ï–ú–£ –ù–ï –ü–†–ò–®–õ–ò (–æ—Ç–∫—Ä—ã—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã):**
–ß—Ç–æ –æ—Å—Ç–∞–ª–æ—Å—å –Ω–µ—Ä–µ—à—ë–Ω–Ω—ã–º.

**–ü–û–†–¢–†–ï–¢–´ –ò –ü–†–û–§–ò–õ–ò:**
–ï—Å–ª–∏ –æ–±—Å—É–∂–¥–∞–ª–∏—Å—å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Ä–æ–ª—è–º - –í–°–ï —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:
- Hard skills, Soft skills, –û–ø—ã—Ç, –õ–∏—á–Ω—ã–µ –∫–∞—á–µ—Å—Ç–≤–∞

**–ò–ù–°–ê–ô–¢–´ –ò –í–´–í–û–î–´:**
–ù–∞–±–ª—é–¥–µ–Ω–∏—è, —Ä–∏—Å–∫–∏, –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏.

**–û–ë–©–ê–Ø –û–¶–ï–ù–ö–ê:**
–ü—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤—Å—Ç—Ä–µ—á–∏.

–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û JSON:
{{
  "summary": "–ü–æ–ª–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ —Å–æ –í–°–ï–ú–ò —Ä–∞–∑–¥–µ–ª–∞–º–∏",
  "key_points": ["15-25 –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤ —Å –¥–µ—Ç–∞–ª—è–º–∏"],
  "action_items": ["–≤—Å–µ –∑–∞–¥–∞—á–∏"]
}}"""

        try:
            response = await self.anthropic.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=16000,
                messages=[{"role": "user", "content": synthesis_prompt}]
            )
            text = response.content[0].text
            start = text.find('{')
            end = text.rfind('}') + 1
            if start != -1 and end > start:
                result = json.loads(text[start:end])
                # Add combined formatted transcript from all chunks
                if all_formatted_transcripts:
                    result["formatted_transcript"] = "\n\n".join(all_formatted_transcripts)
                return result
        except Exception as e:
            logger.error(f"Synthesis failed: {e}")

        # Fallback: just concatenate key points
        result = {
            "summary": f"–î–ª–∏–Ω–Ω—ã–π —Å–æ–∑–≤–æ–Ω ({total_length} —Å–∏–º–≤–æ–ª–æ–≤). –¢–µ–º—ã: " + ", ".join(all_topics[:10]),
            "key_points": all_key_points[:25],
            "action_items": all_action_items[:15]
        }
        # Add combined formatted transcript even in fallback
        if all_formatted_transcripts:
            result["formatted_transcript"] = "\n\n".join(all_formatted_transcripts)
        return result

    async def _analyze_single(self, transcript: str) -> dict:
        """Analyze a single (short) transcript."""
        prompt = f"""–¢—ã –æ–ø—ã—Ç–Ω—ã–π –±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏–∫ —Å –±–µ–∑—É–ø—Ä–µ—á–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º –∫ –¥–µ—Ç–∞–ª—è–º. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - —Å–æ–∑–¥–∞—Ç—å –ò–°–ß–ï–†–ü–´–í–ê–Æ–©–ò–ô –∞–Ω–∞–ª–∏–∑ —Å–æ–∑–≤–æ–Ω–∞.

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:
- –ü—Ä–æ—á–∏—Ç–∞–π –í–ï–°–¨ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –æ—Ç –Ω–∞—á–∞–ª–∞ –¥–æ –∫–æ–Ω—Ü–∞
- –ò–∑–≤–ª–µ–∫–∏ –í–°–ï –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–µ—Ç–∞–ª–∏, –∏–¥–µ–∏, —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
- –ï—Å–ª–∏ –æ–±—Å—É–∂–¥–∞–ª–∏—Å—å –ø–æ—Ä—Ç—Ä–µ—Ç—ã –ª—é–¥–µ–π/—Ä–æ–ª–µ–π - –æ–ø–∏—à–∏ –í–°–ï —É–ø–æ–º—è–Ω—É—Ç—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
- –ù–µ —É–ø—É—Å–∫–∞–π –¥–µ—Ç–∞–ª–∏ –∏–∑ –ª—é–±–æ–π —á–∞—Å—Ç–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ (–Ω–∞—á–∞–ª–æ, —Å–µ—Ä–µ–¥–∏–Ω–∞, –∫–æ–Ω–µ—Ü)
- –õ—É—á—à–µ –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –¥–µ—Ç–∞–ª–µ–π, —á–µ–º —É–ø—É—Å—Ç–∏—Ç—å –≤–∞–∂–Ω–æ–µ

–°–æ–∑–¥–∞–π –∞–Ω–∞–ª–∏–∑ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ:

## SUMMARY:

**–°–£–¢–¨ –†–ê–ó–ì–û–í–û–†–ê** (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è):
–ì–ª–∞–≤–Ω–∞—è —Ü–µ–ª—å –∏ –∫–ª—é—á–µ–≤–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤—Å—Ç—Ä–µ—á–∏.

**–û–°–ù–û–í–ù–ê–Ø –¢–ï–ú–ê:**
–î–µ—Ç–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –≥–ª–∞–≤–Ω–æ–π —Ç–µ–º—ã.

**–ü–û–î–¢–ï–ú–´** (–≤—Å–µ —á—Ç–æ –æ–±—Å—É–∂–¥–∞–ª–æ—Å—å):
–ü–µ—Ä–µ—á–∏—Å–ª–∏ –í–°–ï –ø–æ–¥—Ç–µ–º—ã —Å –∫—Ä–∞—Ç–∫–∏–º –æ–ø–∏—Å–∞–Ω–∏–µ–º –∫–∞–∂–¥–æ–π.

**–î–ï–¢–ê–õ–¨–ù–´–ï –†–ê–ó–ë–û–†–´ –¢–ï–ú:**
–î–ª—è –∫–∞–∂–¥–æ–π –≤–∞–∂–Ω–æ–π —Ç–µ–º—ã/—Ä–æ–ª–∏/–ø—Ä–æ–µ–∫—Ç–∞ —á—Ç–æ –æ–±—Å—É–∂–¥–∞–ª–∏—Å—å –ø–æ–¥—Ä–æ–±–Ω–æ:
- –ö–∞–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è/—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –Ω–∞–∑—ã–≤–∞–ª–∏—Å—å
- –ö–∞–∫–∏–µ –∏–¥–µ–∏ –ø—Ä–µ–¥–ª–∞–≥–∞–ª–∏—Å—å
- –ö–∞–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã –ø—Ä–∏–≤–æ–¥–∏–ª–∏—Å—å
- –ö–∞–∫–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –æ–ø—Ä–µ–¥–µ–ª—è–ª–∏

**–ö –ß–ï–ú–£ –ü–†–ò–®–õ–ò (–¥–æ–≥–æ–≤–æ—Ä—ë–Ω–Ω–æ—Å—Ç–∏):**
–í–°–ï —Ä–µ—à–µ–Ω–∏—è –∏ –¥–æ–≥–æ–≤–æ—Ä—ë–Ω–Ω–æ—Å—Ç–∏, –¥–∞–∂–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ.

**–ö –ß–ï–ú–£ –ù–ï –ü–†–ò–®–õ–ò (–æ—Ç–∫—Ä—ã—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã):**
–ß—Ç–æ –æ—Å—Ç–∞–ª–æ—Å—å –Ω–µ—Ä–µ—à—ë–Ω–Ω—ã–º –∏ —Ç—Ä–µ–±—É–µ—Ç –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –æ–±—Å—É–∂–¥–µ–Ω–∏—è.

**–ü–û–†–¢–†–ï–¢–´ –ò –ü–†–û–§–ò–õ–ò:**
–ï—Å–ª–∏ –æ–±—Å—É–∂–¥–∞–ª–∏—Å—å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞–º, —Ä–æ–ª—è–º, —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞–º - –≤—ã–ø–∏—à–∏ –í–°–ï —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:
- Hard skills
- Soft skills
- –û–ø—ã—Ç
- –õ–∏—á–Ω—ã–µ –∫–∞—á–µ—Å—Ç–≤–∞
- –ß—Ç–æ –¥–æ–ª–∂–µ–Ω —É–º–µ—Ç—å –¥–µ–ª–∞—Ç—å
- –ü—Ä–∏–º–µ—Ä—ã –∏–∑ –æ–±—Å—É–∂–¥–µ–Ω–∏—è

**–ò–ù–°–ê–ô–¢–´ –ò –í–´–í–û–î–´:**
- –°–∫—Ä—ã—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è
- –†–∏—Å–∫–∏ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
- –ß—Ç–æ —á–∏—Ç–∞–µ—Ç—Å—è –º–µ–∂–¥—É —Å—Ç—Ä–æ–∫
- –î–∏–Ω–∞–º–∏–∫–∞ –æ–±—â–µ–Ω–∏—è —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤

**–û–ë–©–ê–Ø –û–¶–ï–ù–ö–ê:**
–ü—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤—Å—Ç—Ä–µ—á–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏.

## KEY_POINTS (15-25 –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤):
–ö–∞–∂–¥—ã–π –ø—É–Ω–∫—Ç –ö–û–ù–ö–†–ï–¢–ù–´–ô —Å –¥–µ—Ç–∞–ª—è–º–∏, –∏–º–µ–Ω–∞–º–∏, —Ü–∏—Ñ—Ä–∞–º–∏, –ø—Ä–∏–º–µ—Ä–∞–º–∏.
–û—Ö–≤–∞—Ç–∏ –í–°–ï —á–∞—Å—Ç–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ - –Ω–∞—á–∞–ª–æ, —Å–µ—Ä–µ–¥–∏–Ω—É –∏ –∫–æ–Ω–µ—Ü!

## ACTION_ITEMS:
–í—Å–µ –∑–∞–¥–∞—á–∏ —Å –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∏ —Å—Ä–æ–∫–∞–º–∏.

## FORMATTED_TRANSCRIPT (–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º):
–ü—Ä–µ–æ–±—Ä–∞–∑—É–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –≤ —á–∏—Ç–∞–µ–º—ã–π –¥–∏–∞–ª–æ–≥ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —Å–ø–∏–∫–µ—Ä–æ–≤.
–í–ê–ñ–ù–û:
- –û–ø—Ä–µ–¥–µ–ª–∏ —Å–ø–∏–∫–µ—Ä–æ–≤ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É (–≤–æ–ø—Ä–æ—Å—ã vs –æ—Ç–≤–µ—Ç—ã, —Å—Ç–∏–ª—å —Ä–µ—á–∏)
- –ò—Å–ø–æ–ª—å–∑—É–π –ø–æ–Ω—è—Ç–Ω—ã–µ —Ä–æ–ª–∏: "HR:", "–ö–∞–Ω–¥–∏–¥–∞—Ç:", "–ú–µ–Ω–µ–¥–∂–µ—Ä:", "–°–ø–∏–∫–µ—Ä 1:", "–°–ø–∏–∫–µ—Ä 2:" –∏ —Ç.–¥.
- –ö–∞–∂–¥–∞—è —Ä–µ–ø–ª–∏–∫–∞ —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏
- –°–æ—Ö—Ä–∞–Ω–∏ –í–°–ï —Å–ª–æ–≤–∞ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞, —Ç–æ–ª—å–∫–æ –¥–æ–±–∞–≤—å –º–µ—Ç–∫–∏ —Å–ø–∏–∫–µ—Ä–æ–≤
- –§–æ—Ä–º–∞—Ç: "–°–ø–∏–∫–µ—Ä: —Ç–µ–∫—Å—Ç —Ä–µ–ø–ª–∏–∫–∏"

–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç:
---
{transcript}
---

–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–º JSON:
{{
  "summary": "–ü–æ–ª–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ —Å–æ –í–°–ï–ú–ò —Ä–∞–∑–¥–µ–ª–∞–º–∏ (–∏—Å–ø–æ–ª—å–∑—É–π **–∂–∏—Ä–Ω—ã–π** –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤, \\n –¥–ª—è –ø–µ—Ä–µ–Ω–æ—Å–æ–≤)",
  "key_points": ["–î–µ—Ç–∞–ª—å–Ω—ã–π –ø—É–Ω–∫—Ç 1", "–î–µ—Ç–∞–ª—å–Ω—ã–π –ø—É–Ω–∫—Ç 2", ...],
  "action_items": ["–ó–∞–¥–∞—á–∞: –∫—Ç–æ, —á—Ç–æ, –∫–æ–≥–¥–∞", ...],
  "formatted_transcript": "HR: –•–æ—Ä–æ—à–æ, –¥–∞–≤–∞–π –ø–µ—Ä–µ–π–¥–µ–º –∫ —Ç–≤–æ–µ–º—É –æ–ø—ã—Ç—É...\\n–ö–∞–Ω–¥–∏–¥–∞—Ç: –î–∞, —è —Å–µ–π—á–∞—Å —Å—Ç—É–¥–µ–Ω—Ç...\\nHR: –ê —Ç—ã —É—á–∏—à—å—Å—è –æ—á–Ω–æ?\\n–ö–∞–Ω–¥–∏–¥–∞—Ç: –í–µ—á–µ—Ä–Ω—è—è —Ñ–æ—Ä–º–∞..."
}}"""

        try:
            response = await self.anthropic.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=16000,
                messages=[{"role": "user", "content": prompt}]
            )

            text = response.content[0].text

            # Parse JSON from response
            start = text.find('{')
            end = text.rfind('}') + 1
            if start != -1 and end > start:
                return json.loads(text[start:end])

        except Exception as e:
            logger.error(f"Analysis failed: {e}")

        return {
            "summary": transcript[:500] + "..." if len(transcript) > 500 else transcript,
            "key_points": [],
            "action_items": []
        }


# Global instance
call_processor = CallProcessor()


async def process_call_background(call_id: int):
    """Background task wrapper for call processing."""
    await call_processor.process_call(call_id)


# =============================================================================
# SMART CONTEXT FUNCTIONS FOR AI ANALYSIS
# =============================================================================

# Segment duration in seconds (10 minutes)
SEGMENT_DURATION = 600

# Short call threshold - use full transcript if under this (30 minutes)
SHORT_CALL_THRESHOLD = 1800


def calculate_speaker_stats(speakers: list) -> dict:
    """
    Calculate statistics for each speaker from call segments.

    Args:
        speakers: List of speaker segments [{speaker, start, end, text}, ...]

    Returns:
        Dict mapping speaker name to stats
    """
    if not speakers:
        return {}

    stats = {}
    # Average speaking rate: ~150 words per minute, ~5 characters per word
    # So roughly 750 characters per minute = 12.5 chars per second
    CHARS_PER_SECOND = 12.5

    # Check if timestamps are available
    has_real_timestamps = any(
        (segment.get("start", 0) or 0) > 0 or (segment.get("end", 0) or 0) > 0
        for segment in speakers
    )

    for segment in speakers:
        speaker = segment.get("speaker", "Unknown")
        start = segment.get("start", 0) or 0
        end = segment.get("end", 0) or 0
        text = segment.get("text", "") or ""

        # Calculate duration - use timestamps if available, otherwise estimate from text length
        if has_real_timestamps:
            duration = max(0, end - start)
        else:
            # Estimate duration based on text length
            duration = len(text) / CHARS_PER_SECOND if text else 0

        # Count words in text
        word_count = len(text.split()) if text else 0

        if speaker not in stats:
            stats[speaker] = {
                "total_seconds": 0,
                "talktime_seconds": 0,  # Alias for UI compatibility
                "segment_count": 0,
                "first_speak_time": start,
                "last_speak_time": end,
                "total_chars": 0,  # Track chars for percentage calculation
                "total_words": 0,  # Track words for WPM calculation
                "estimated": not has_real_timestamps  # Flag if estimated
            }

        stats[speaker]["total_seconds"] += duration
        stats[speaker]["talktime_seconds"] += duration
        stats[speaker]["segment_count"] += 1
        stats[speaker]["total_chars"] += len(text)
        stats[speaker]["total_words"] += word_count
        if has_real_timestamps:
            stats[speaker]["first_speak_time"] = min(stats[speaker]["first_speak_time"], start)
            stats[speaker]["last_speak_time"] = max(stats[speaker]["last_speak_time"], end)

    # Calculate averages, percentages, and WPM
    total_seconds_all = sum(s["total_seconds"] for s in stats.values())
    for speaker, data in stats.items():
        if data["segment_count"] > 0:
            data["avg_segment_length"] = data["total_seconds"] / data["segment_count"]
        else:
            data["avg_segment_length"] = 0

        # Add percentage of total speaking time
        if total_seconds_all > 0:
            data["percentage"] = round(data["total_seconds"] / total_seconds_all * 100, 1)
            data["talktime_percent"] = int(data["total_seconds"] / total_seconds_all * 100)
        else:
            data["percentage"] = 0
            data["talktime_percent"] = 0

        # Calculate WPM (words per minute)
        if data["total_seconds"] > 0:
            minutes = data["total_seconds"] / 60
            data["wpm"] = int(data["total_words"] / minutes) if minutes > 0 else 0
        else:
            data["wpm"] = 0

    return stats


async def identify_participant_roles(call, db) -> dict:
    """
    Identify roles of call participants.

    Returns:
        {
            "evaluator": {"user_id": 5, "name": "HR Manager", "speaker_name": "..."},
            "target": {"entity_id": 12, "name": "Candidate", "type": "candidate", "speaker_name": "..."},
            "others": [{"name": "...", "speaker_name": "...", "role": "unknown"}]
        }
    """
    from sqlalchemy import select
    from ..models.database import User, Entity

    roles = {
        "evaluator": None,
        "target": None,
        "others": []
    }

    if not call.speakers:
        return roles

    # Get call owner as potential evaluator
    if call.owner_id:
        result = await db.execute(select(User).where(User.id == call.owner_id))
        owner = result.scalar_one_or_none()
        if owner:
            roles["evaluator"] = {
                "user_id": owner.id,
                "name": owner.name,
                "email": owner.email,
                "speaker_name": None
            }

    # Get target entity
    if call.entity_id:
        result = await db.execute(select(Entity).where(Entity.id == call.entity_id))
        entity = result.scalar_one_or_none()
        if entity:
            roles["target"] = {
                "entity_id": entity.id,
                "name": entity.name,
                "type": entity.type.value if entity.type else None,
                "email": entity.email,
                "speaker_name": None
            }

    # Try to match speakers to roles by email
    speaker_stats = calculate_speaker_stats(call.speakers)

    for speaker_name in speaker_stats.keys():
        # Extract email from speaker name if present
        email = None
        if "@" in speaker_name:
            parts = speaker_name.split()
            for part in parts:
                if "@" in part:
                    email = part.strip("()<>[]\"'").lower()
                    break

        matched = False

        # Match evaluator by email
        if roles["evaluator"] and email:
            evaluator_email = (roles["evaluator"].get("email") or "").lower()
            if evaluator_email and evaluator_email == email:
                roles["evaluator"]["speaker_name"] = speaker_name
                matched = True

        # Match target by email
        if not matched and roles["target"] and email:
            target_email = (roles["target"].get("email") or "").lower()
            if target_email and target_email == email:
                roles["target"]["speaker_name"] = speaker_name
                matched = True

        # Unmatched speakers go to others
        if not matched:
            roles["others"].append({
                "name": speaker_name,
                "speaker_name": speaker_name,
                "role": "unknown"
            })

    return roles


async def process_call_for_ai(call, db, force_reprocess: bool = False):
    """
    Process call recording for efficient AI analysis.

    Calculates speaker stats and identifies participant roles.
    """
    # Skip if already processed (unless forced)
    if not force_reprocess and call.speaker_stats and call.participant_roles:
        logger.debug(f"Call {call.id} already processed, skipping")
        return call

    logger.info(f"Processing call {call.id} for AI analysis")

    # Calculate speaker stats
    if call.speakers:
        call.speaker_stats = calculate_speaker_stats(call.speakers)
        logger.debug(f"Calculated stats for {len(call.speaker_stats)} speakers")

    # Identify participant roles
    call.participant_roles = await identify_participant_roles(call, db)

    # Save changes
    db.add(call)
    await db.commit()
    await db.refresh(call)

    return call


def build_smart_context(call, include_full_transcript: bool = False) -> str:
    """
    Build optimized context for AI analysis.

    For short calls (< 30 min): includes full transcript
    For long calls: includes summary + smart truncated transcript + participant info
    """
    parts = []

    # Call metadata
    call_date = call.created_at.strftime('%d.%m.%Y') if call.created_at else "–¥–∞—Ç–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞"
    parts.append(f"### –ó–≤–æ–Ω–æ–∫ –æ—Ç {call_date}")

    if call.title:
        parts.append(f"**–ù–∞–∑–≤–∞–Ω–∏–µ:** {call.title}")

    if call.duration_seconds:
        mins = call.duration_seconds // 60
        secs = call.duration_seconds % 60
        parts.append(f"**–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:** {mins}–º {secs}—Å")

    # Participant roles with speaking time
    if call.participant_roles:
        parts.append("\n**–£—á–∞—Å—Ç–Ω–∏–∫–∏:**")

        evaluator = call.participant_roles.get("evaluator")
        if evaluator:
            speaker_name = evaluator.get("speaker_name", "")
            time_str = ""
            if speaker_name and call.speaker_stats:
                stats = call.speaker_stats.get(speaker_name, {})
                total_secs = stats.get("total_seconds", 0)
                if total_secs:
                    time_str = f" (~{int(total_secs // 60)}–º)"
            parts.append(f"- üîë {evaluator.get('name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')} (–æ—Ü–µ–Ω–∏–≤–∞–µ—Ç){time_str}")

        target = call.participant_roles.get("target")
        if target:
            speaker_name = target.get("speaker_name", "")
            time_str = ""
            if speaker_name and call.speaker_stats:
                stats = call.speaker_stats.get(speaker_name, {})
                total_secs = stats.get("total_seconds", 0)
                if total_secs:
                    time_str = f" (~{int(total_secs // 60)}–º)"
            entity_type = target.get("type", "–∫–æ–Ω—Ç–∞–∫—Ç")
            parts.append(f"- üéØ {target.get('name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')} ({entity_type}){time_str}")

        for other in call.participant_roles.get("others", []):
            speaker_name = other.get("speaker_name", "")
            time_str = ""
            if speaker_name and call.speaker_stats:
                stats = call.speaker_stats.get(speaker_name, {})
                total_secs = stats.get("total_seconds", 0)
                if total_secs:
                    time_str = f" (~{int(total_secs // 60)}–º)"
            parts.append(f"- üë§ {other.get('name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}{time_str}")

    elif call.speaker_stats:
        # Fallback to speaker stats if no roles identified
        parts.append("\n**–£—á–∞—Å—Ç–Ω–∏–∫–∏ –∑–≤–æ–Ω–∫–∞:**")
        for speaker, stats in sorted(call.speaker_stats.items(), key=lambda x: -x[1].get("total_seconds", 0)):
            total_secs = stats.get("total_seconds", 0)
            mins = int(total_secs // 60)
            secs = int(total_secs % 60)
            parts.append(f"- {speaker}: ~{mins}–º {secs}—Å")

    # Summary
    if call.summary:
        parts.append(f"\n**–°–∞–º–º–∞—Ä–∏:** {call.summary}")

    # Key points
    if call.key_points:
        parts.append("\n**–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:**")
        for point in call.key_points[:10]:
            parts.append(f"- {point}")

    # Transcript handling
    is_short_call = not call.duration_seconds or call.duration_seconds <= SHORT_CALL_THRESHOLD

    if call.transcript:
        if include_full_transcript or is_short_call:
            # Include full transcript for short calls
            parts.append(f"\n**–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç:**\n{call.transcript}")
        else:
            # For long calls, use smart truncation
            MAX_CHARS = 40000
            if len(call.transcript) > MAX_CHARS:
                first_part = int(MAX_CHARS * 0.6)
                last_part = MAX_CHARS - first_part
                transcript = (
                    call.transcript[:first_part] +
                    f"\n\n... [–ø—Ä–æ–ø—É—â–µ–Ω–æ ~{(len(call.transcript) - MAX_CHARS) // 1000}k —Å–∏–º–≤–æ–ª–æ–≤] ...\n\n" +
                    call.transcript[-last_part:]
                )
            else:
                transcript = call.transcript
            parts.append(f"\n**–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç:**\n{transcript}")

    return "\n".join(parts)
